# ğŸ“š PDFâ€‘based Retrievalâ€‘Augmented Generation (RAG)

A powerful **Retrieval-Augmented Generation (RAG)** system that allows users to upload PDF documents and ask questions about their content. Powered by **Google Gemini**, **Qdrant**, and **FastAPI**.

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.109.0-009688)
![Streamlit](https://img.shields.io/badge/Streamlit-1.31.0-FF4B4B)
![Gemini](https://img.shields.io/badge/AI-Gemini-8E75B2)

## ğŸš€ Features

- **ğŸ“„ PDF Ingestion:** Upload and process PDF documents automatically.
- **ğŸ§  Semantic Search:** Uses high-performance vector embeddings to find relevant context.
- **ğŸ¤– Generative AI:** Generates accurate answers using Google's Gemini models.
- **âš¡ Real-time:** Fast retrieval and response generation.
- **ğŸ³ Dockerized:** Easy infrastructure setup with Docker Compose.

## ğŸ“‚ Project Structure

The project is organized into a backend API and a frontend user interface.

```plaintext
.
â”œâ”€â”€ backend/                    # ğŸ FastAPI Backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ db/                 # Database configuration & models
â”‚   â”‚   â”‚   â”œâ”€â”€ crud.py         # Database CRUD operations
â”‚   â”‚   â”‚   â”œâ”€â”€ init_db.py      # Database initialization
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py       # SQLAlchemy models
â”‚   â”‚   â”‚   â””â”€â”€ session.py      # Database session management
â”‚   â”‚   â”œâ”€â”€ routes/             # API Endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py         # Chat/Question answering endpoints
â”‚   â”‚   â”‚   â””â”€â”€ ingest.py       # Document upload endpoints
â”‚   â”‚   â”œâ”€â”€ schemas/            # Pydantic Data Schemas
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py         # Chat request/response schemas
â”‚   â”‚   â”‚   â””â”€â”€ ingest.py       # Ingestion schemas
â”‚   â”‚   â”œâ”€â”€ services/           # Core Business Logic
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py          # Gemini LLM integration
â”‚   â”‚   â”‚   â”œâ”€â”€ pdf_ingest.py   # PDF parsing logic
â”‚   â”‚   â”‚   â””â”€â”€ retriever.py    # Qdrant vector search logic
â”‚   â”‚   â”œâ”€â”€ utils/              # Helper Utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ logger.py       # Logging configuration
â”‚   â”‚   â”‚   â”œâ”€â”€ text_splitter.py# Text chunking utilities
â”‚   â”‚   â”‚   â””â”€â”€ timing.py       # Performance timing decorators
â”‚   â”‚   â”œâ”€â”€ config.py           # Application configuration
â”‚   â”‚   â””â”€â”€ main.py             # App entry point
â”‚   â””â”€â”€ requirements.txt        # Backend dependencies
â”‚
â”œâ”€â”€ frontend/                   # ğŸ–¥ï¸ Streamlit Frontend
â”‚   â”œâ”€â”€ api.py                  # API client for communicating with backend
â”‚   â”œâ”€â”€ streamlit_app.py        # Main Streamlit application UI
â”‚   â””â”€â”€ requirements.txt        # Frontend dependencies
â”‚
â”œâ”€â”€ docker-compose.infra.yml    # ğŸ³ Infrastructure (Postgres, Qdrant)
â”œâ”€â”€ starter.py                  # ğŸš€ Helper script to launch everything
â”œâ”€â”€ .env.example                # ğŸ” Environment variable template
â””â”€â”€ README.md                   # ğŸ“– Project documentation
```

## ğŸ› ï¸ Tech Stack

- **Backend:** FastAPI, SQLAlchemy, Pydantic
- **Frontend:** Streamlit
- **AI/LLM:** Google Gemini (via `google-generativeai`)
- **Vector DB:** Qdrant
- **Database:** PostgreSQL
- **Infrastructure:** Docker, Docker Compose

## âš¡ Quick Start

### 1. Prerequisites

- Python 3.10+
- Docker & Docker Compose

### 2. Clone & Configure

```bash
git clone https://github.com/devanshuuu-git/AlgorithmX-assessment.git
cd AlgorithmX-assessment

# Create .env file
cp .env.example .env
```

**Important:** Open `.env` and add your `GEMINI_API_KEY`.

### 3. Run with One Command

We provide a starter script to set up the infrastructure and run both services.

```bash
python starter.py
```

This will:
1. Start Postgres & Qdrant containers.
2. Launch the FastAPI backend (port 8000).
3. Launch the Streamlit frontend (port 8501).

---

## ğŸ”§ Manual Setup (Optional)

If you prefer running services individually:

**1. Start Infrastructure:**
```bash
docker-compose -f docker-compose.infra.yml up -d
```

**2. Backend Setup:**
```bash
cd backend
python -m venv venv
# Windows:
.\venv\Scripts\activate
# Mac/Linux:
# source venv/bin/activate

pip install -r requirements.txt
uvicorn app.main:app --reload
```

**3. Frontend Setup:**
```bash
cd frontend
python -m venv venv
# Windows:
.\venv\Scripts\activate
# Mac/Linux:
# source venv/bin/activate

pip install -r requirements.txt
streamlit run streamlit_app.py
```

## ğŸ“ Usage Guide

1.  **Access the App:** Go to `http://localhost:8501`.
2.  **Upload:** Use the sidebar to upload a PDF file.
3.  **Process:** Click "Process PDF" to ingest the document into the vector database.
4.  **Ask:** Type your question in the chat input. The AI will answer based on the PDF content.

## ğŸ¤ Contributing

1. Fork the repository.
2. Create a new branch (`git checkout -b feature/amazing-feature`).
3. Commit your changes (`git commit -m 'Add some amazing feature'`).
4. Push to the branch (`git push origin feature/amazing-feature`).
5. Open a Pull Request.
